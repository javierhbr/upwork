{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ž IVR Call Pattern Analysis with Local LLM\n",
    "\n",
    "This notebook implements a complete system to:\n",
    "1. Load and process self-service phone call data\n",
    "2. Generate local embeddings using Ollama\n",
    "3. Store in ChromaDB (local vector database)\n",
    "4. Detect failure patterns and explain them with LLM\n",
    "\n",
    "**Requirements:**\n",
    "- Ollama installed and running locally\n",
    "- Models downloaded (e.g.: `ollama pull llama3.1` and `ollama pull nomic-embed-text`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install chromadb pandas ollama scikit-learn matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import ollama\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ollama and ChromaDB Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Centralized system configuration\"\"\"\n",
    "    # Ollama Models\n",
    "    EMBEDDING_MODEL: str = \"nomic-embed-text\"  # Model for embeddings\n",
    "    LLM_MODEL: str = \"llama3.1\"                # Model for analysis/explanations\n",
    "    \n",
    "    # ChromaDB\n",
    "    CHROMA_PERSIST_DIR: str = \"./chroma_ivr_db\"\n",
    "    COLLECTION_NAME: str = \"ivr_call_patterns\"\n",
    "    \n",
    "    # Analysis\n",
    "    TOP_K_SIMILAR: int = 5  # Number of similar cases to retrieve\n",
    "    \n",
    "config = Config()\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   - Embedding model: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"   - LLM model: {config.LLM_MODEL}\")\n",
    "print(f\"   - ChromaDB directory: {config.CHROMA_PERSIST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Ollama connection\n",
    "def verify_ollama_connection():\n",
    "    \"\"\"Verify that Ollama is running and check available models\"\"\"\n",
    "    try:\n",
    "        models = ollama.list()\n",
    "        model_names = [m['name'].split(':')[0] for m in models.get('models', [])]\n",
    "        print(\"âœ… Ollama connected successfully\")\n",
    "        print(f\"   Available models: {model_names}\")\n",
    "        \n",
    "        # Check required models\n",
    "        required = [config.EMBEDDING_MODEL, config.LLM_MODEL]\n",
    "        for model in required:\n",
    "            if model not in model_names:\n",
    "                print(f\"âš ï¸  Model '{model}' not found. Run: ollama pull {model}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error connecting to Ollama: {e}\")\n",
    "        print(\"   Make sure Ollama is running: ollama serve\")\n",
    "        return False\n",
    "\n",
    "verify_ollama_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Dataset Generation\n",
    "\n",
    "If you already have your CSV, skip this cell and load your file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_dataset(n_calls: int = 500) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a sample dataset simulating IVR calls.\n",
    "    Replace this with your actual CSV.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define IVR flow steps\n",
    "    ivr_steps = [\n",
    "        \"welcome_message\",\n",
    "        \"language_selection\",\n",
    "        \"authentication\",\n",
    "        \"main_menu\",\n",
    "        \"inquiry_submenu\",\n",
    "        \"data_entry\",\n",
    "        \"data_validation\",\n",
    "        \"processing\",\n",
    "        \"confirmation\",\n",
    "        \"goodbye\"\n",
    "    ]\n",
    "    \n",
    "    # Possible errors by step\n",
    "    error_types = {\n",
    "        \"authentication\": [\"auth_timeout\", \"invalid_credentials\", \"account_locked\", \"biometric_fail\"],\n",
    "        \"data_entry\": [\"dtmf_timeout\", \"invalid_input\", \"speech_not_recognized\", \"too_many_retries\"],\n",
    "        \"data_validation\": [\"data_mismatch\", \"expired_data\", \"system_unavailable\"],\n",
    "        \"processing\": [\"backend_timeout\", \"service_unavailable\", \"transaction_failed\"],\n",
    "        \"main_menu\": [\"no_input\", \"invalid_option\", \"confusion_detected\"]\n",
    "    }\n",
    "    \n",
    "    calls = []\n",
    "    \n",
    "    for i in range(n_calls):\n",
    "        call_id = f\"CALL_{i:05d}\"\n",
    "        \n",
    "        # Determine if call will be successful (60% success rate)\n",
    "        is_success = np.random.random() < 0.6\n",
    "        \n",
    "        if is_success:\n",
    "            # Successful call - completes all steps\n",
    "            steps_completed = ivr_steps.copy()\n",
    "            step_results = {step: \"success\" for step in steps_completed}\n",
    "            final_result = \"success\"\n",
    "            failure_step = None\n",
    "            failure_reason = None\n",
    "            end_action = \"completed\"\n",
    "        else:\n",
    "            # Failed call - fails at some step\n",
    "            fail_step_idx = np.random.choice([2, 3, 4, 5, 6, 7], p=[0.25, 0.1, 0.1, 0.25, 0.15, 0.15])\n",
    "            steps_completed = ivr_steps[:fail_step_idx + 1]\n",
    "            \n",
    "            step_results = {step: \"success\" for step in steps_completed[:-1]}\n",
    "            failure_step = steps_completed[-1]\n",
    "            \n",
    "            # Assign error type\n",
    "            if failure_step in error_types:\n",
    "                failure_reason = np.random.choice(error_types[failure_step])\n",
    "            else:\n",
    "                failure_reason = \"unknown_error\"\n",
    "            \n",
    "            step_results[failure_step] = \"error\"\n",
    "            final_result = \"error\"\n",
    "            \n",
    "            # Final action\n",
    "            end_action = np.random.choice([\"hangup\", \"transfer_agent\"], p=[0.4, 0.6])\n",
    "        \n",
    "        # Additional metadata\n",
    "        call_duration = np.random.randint(30, 300) if is_success else np.random.randint(15, 180)\n",
    "        retries = 0 if is_success else np.random.randint(1, 4)\n",
    "        \n",
    "        calls.append({\n",
    "            \"call_id\": call_id,\n",
    "            \"timestamp\": pd.Timestamp.now() - pd.Timedelta(days=np.random.randint(0, 30)),\n",
    "            \"steps_completed\": json.dumps(steps_completed),\n",
    "            \"step_results\": json.dumps(step_results),\n",
    "            \"final_result\": final_result,\n",
    "            \"failure_step\": failure_step,\n",
    "            \"failure_reason\": failure_reason,\n",
    "            \"end_action\": end_action,\n",
    "            \"call_duration_seconds\": call_duration,\n",
    "            \"retry_count\": retries,\n",
    "            \"customer_segment\": np.random.choice([\"premium\", \"standard\", \"basic\"]),\n",
    "            \"call_type\": np.random.choice([\"billing\", \"support\", \"sales\", \"account_info\"]),\n",
    "            \"hour_of_day\": np.random.randint(8, 22),\n",
    "            \"day_of_week\": np.random.choice([\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\"])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(calls)\n",
    "\n",
    "# Generate sample dataset\n",
    "df = generate_sample_dataset(500)\n",
    "print(f\"ðŸ“Š Dataset generated: {len(df)} calls\")\n",
    "print(f\"   - Successful: {len(df[df['final_result'] == 'success'])}\")\n",
    "print(f\"   - Failed: {len(df[df['final_result'] == 'error'])}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â¬‡ï¸ ALTERNATIVE: Load your own CSV\n",
    "# Uncomment and adjust according to your data structure\n",
    "\n",
    "# df = pd.read_csv(\"your_file.csv\")\n",
    "# print(f\"Dataset loaded: {len(df)} records\")\n",
    "# print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing and Text Representation Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallDataProcessor:\n",
    "    \"\"\"\n",
    "    Processes call data and generates text representations\n",
    "    to create meaningful embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.processed_calls = []\n",
    "    \n",
    "    def create_call_narrative(self, row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Converts a DataFrame row into a textual narrative\n",
    "        that captures the complete call context.\n",
    "        \"\"\"\n",
    "        # Parse JSON if necessary\n",
    "        if isinstance(row['steps_completed'], str):\n",
    "            steps = json.loads(row['steps_completed'])\n",
    "        else:\n",
    "            steps = row['steps_completed']\n",
    "            \n",
    "        if isinstance(row['step_results'], str):\n",
    "            results = json.loads(row['step_results'])\n",
    "        else:\n",
    "            results = row['step_results']\n",
    "        \n",
    "        # Build narrative\n",
    "        narrative_parts = [\n",
    "            f\"Call type: {row['call_type']}\",\n",
    "            f\"Customer segment: {row['customer_segment']}\",\n",
    "            f\"Day: {row['day_of_week']}, Hour: {row['hour_of_day']}:00\",\n",
    "            f\"Duration: {row['call_duration_seconds']} seconds\",\n",
    "            \"\\nStep flow:\"\n",
    "        ]\n",
    "        \n",
    "        # Describe each step\n",
    "        for i, step in enumerate(steps, 1):\n",
    "            result = results.get(step, 'unknown')\n",
    "            status = \"âœ“\" if result == \"success\" else \"âœ—\"\n",
    "            narrative_parts.append(f\"  {i}. {step}: {status} ({result})\")\n",
    "        \n",
    "        # Final result\n",
    "        narrative_parts.append(f\"\\nFinal result: {row['final_result'].upper()}\")\n",
    "        \n",
    "        if row['final_result'] == 'error':\n",
    "            narrative_parts.extend([\n",
    "                f\"Failure step: {row['failure_step']}\",\n",
    "                f\"Failure reason: {row['failure_reason']}\",\n",
    "                f\"End action: {row['end_action']}\",\n",
    "                f\"Retry attempts: {row['retry_count']}\"\n",
    "            ])\n",
    "        \n",
    "        return \"\\n\".join(narrative_parts)\n",
    "    \n",
    "    def create_pattern_signature(self, row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Creates a more concise pattern signature for quick searching.\n",
    "        \"\"\"\n",
    "        if isinstance(row['steps_completed'], str):\n",
    "            steps = json.loads(row['steps_completed'])\n",
    "        else:\n",
    "            steps = row['steps_completed']\n",
    "        \n",
    "        signature_parts = [\n",
    "            f\"result:{row['final_result']}\",\n",
    "            f\"steps:{len(steps)}\",\n",
    "            f\"type:{row['call_type']}\",\n",
    "            f\"segment:{row['customer_segment']}\"\n",
    "        ]\n",
    "        \n",
    "        if row['final_result'] == 'error':\n",
    "            signature_parts.extend([\n",
    "                f\"fail_step:{row['failure_step']}\",\n",
    "                f\"fail_reason:{row['failure_reason']}\",\n",
    "                f\"end_action:{row['end_action']}\"\n",
    "            ])\n",
    "        \n",
    "        return \" | \".join(signature_parts)\n",
    "    \n",
    "    def process_all(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Processes all calls and returns list of documents.\n",
    "        \"\"\"\n",
    "        self.processed_calls = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            doc = {\n",
    "                \"id\": row['call_id'],\n",
    "                \"narrative\": self.create_call_narrative(row),\n",
    "                \"signature\": self.create_pattern_signature(row),\n",
    "                \"metadata\": {\n",
    "                    \"call_id\": row['call_id'],\n",
    "                    \"final_result\": row['final_result'],\n",
    "                    \"failure_step\": row['failure_step'] if pd.notna(row['failure_step']) else None,\n",
    "                    \"failure_reason\": row['failure_reason'] if pd.notna(row['failure_reason']) else None,\n",
    "                    \"call_type\": row['call_type'],\n",
    "                    \"customer_segment\": row['customer_segment'],\n",
    "                    \"end_action\": row['end_action']\n",
    "                }\n",
    "            }\n",
    "            self.processed_calls.append(doc)\n",
    "        \n",
    "        return self.processed_calls\n",
    "\n",
    "# Process data\n",
    "processor = CallDataProcessor(df)\n",
    "processed_calls = processor.process_all()\n",
    "\n",
    "print(f\"âœ… {len(processed_calls)} calls processed\")\n",
    "print(\"\\nðŸ“ Example of generated narrative:\")\n",
    "print(\"-\" * 50)\n",
    "# Show example of failed call\n",
    "failed_example = next(c for c in processed_calls if c['metadata']['final_result'] == 'error')\n",
    "print(failed_example['narrative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Generation with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaEmbeddings:\n",
    "    \"\"\"\n",
    "    Generates embeddings using Ollama locally.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"nomic-embed-text\"):\n",
    "        self.model = model\n",
    "    \n",
    "    def embed_single(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for a single text.\"\"\"\n",
    "        response = ollama.embeddings(\n",
    "            model=self.model,\n",
    "            prompt=text\n",
    "        )\n",
    "        return response['embedding']\n",
    "    \n",
    "    def embed_batch(self, texts: List[str], show_progress: bool = True) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for multiple texts.\"\"\"\n",
    "        embeddings = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            emb = self.embed_single(text)\n",
    "            embeddings.append(emb)\n",
    "            \n",
    "            if show_progress and (i + 1) % 50 == 0:\n",
    "                print(f\"   Processed: {i + 1}/{total}\")\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Initialize embedding generator\n",
    "embedder = OllamaEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "\n",
    "# Quick test\n",
    "test_emb = embedder.embed_single(\"embedding test\")\n",
    "print(f\"âœ… Embeddings working - Dimension: {len(test_emb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ChromaDB Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IVRVectorStore:\n",
    "    \"\"\"\n",
    "    Manages vector storage for IVR calls using ChromaDB.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, persist_dir: str, collection_name: str, embedder: OllamaEmbeddings):\n",
    "        self.embedder = embedder\n",
    "        \n",
    "        # Initialize ChromaDB with persistence\n",
    "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
    "        \n",
    "        # Create or retrieve collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"IVR call patterns for failure analysis\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ChromaDB initialized\")\n",
    "        print(f\"   Collection: {collection_name}\")\n",
    "        print(f\"   Existing documents: {self.collection.count()}\")\n",
    "    \n",
    "    def add_calls(self, processed_calls: List[Dict], batch_size: int = 100):\n",
    "        \"\"\"\n",
    "        Add processed calls to the vector database.\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ“¥ Indexing {len(processed_calls)} calls...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        \n",
    "        for call in processed_calls:\n",
    "            ids.append(call['id'])\n",
    "            # Combine narrative and signature for the document\n",
    "            documents.append(f\"{call['narrative']}\\n\\nPattern: {call['signature']}\")\n",
    "            metadatas.append(call['metadata'])\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(\"   Generating embeddings...\")\n",
    "        embeddings = self.embedder.embed_batch(documents)\n",
    "        \n",
    "        # Insert in batches\n",
    "        for i in range(0, len(ids), batch_size):\n",
    "            end_idx = min(i + batch_size, len(ids))\n",
    "            \n",
    "            self.collection.add(\n",
    "                ids=ids[i:end_idx],\n",
    "                documents=documents[i:end_idx],\n",
    "                embeddings=embeddings[i:end_idx],\n",
    "                metadatas=metadatas[i:end_idx]\n",
    "            )\n",
    "        \n",
    "        print(f\"âœ… {len(ids)} calls indexed successfully\")\n",
    "        print(f\"   Total in collection: {self.collection.count()}\")\n",
    "    \n",
    "    def search_similar(self, query_text: str, n_results: int = 5, \n",
    "                       filter_result: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Search for calls similar to a query.\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedder.embed_single(query_text)\n",
    "        \n",
    "        # Build optional filter\n",
    "        where_filter = None\n",
    "        if filter_result:\n",
    "            where_filter = {\"final_result\": filter_result}\n",
    "        \n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            where=where_filter,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_failure_patterns(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get failure pattern statistics.\n",
    "        \"\"\"\n",
    "        # Get all error documents\n",
    "        results = self.collection.get(\n",
    "            where={\"final_result\": \"error\"},\n",
    "            include=[\"metadatas\"]\n",
    "        )\n",
    "        \n",
    "        # Analyze patterns\n",
    "        failure_steps = Counter()\n",
    "        failure_reasons = Counter()\n",
    "        end_actions = Counter()\n",
    "        \n",
    "        for meta in results['metadatas']:\n",
    "            if meta.get('failure_step'):\n",
    "                failure_steps[meta['failure_step']] += 1\n",
    "            if meta.get('failure_reason'):\n",
    "                failure_reasons[meta['failure_reason']] += 1\n",
    "            if meta.get('end_action'):\n",
    "                end_actions[meta['end_action']] += 1\n",
    "        \n",
    "        return {\n",
    "            \"total_failures\": len(results['metadatas']),\n",
    "            \"by_step\": dict(failure_steps.most_common()),\n",
    "            \"by_reason\": dict(failure_reasons.most_common()),\n",
    "            \"by_end_action\": dict(end_actions.most_common())\n",
    "        }\n",
    "    \n",
    "    def clear_collection(self):\n",
    "        \"\"\"Clear the collection (useful for re-training).\"\"\"\n",
    "        self.client.delete_collection(self.collection.name)\n",
    "        self.collection = self.client.create_collection(\n",
    "            name=self.collection.name,\n",
    "            metadata={\"description\": \"IVR call patterns for failure analysis\"}\n",
    "        )\n",
    "        print(\"ðŸ—‘ï¸ Collection cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "vector_store = IVRVectorStore(\n",
    "    persist_dir=config.CHROMA_PERSIST_DIR,\n",
    "    collection_name=config.COLLECTION_NAME,\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "# Clear if we want to start fresh (optional)\n",
    "# vector_store.clear_collection()\n",
    "\n",
    "# Index calls only if collection is empty\n",
    "if vector_store.collection.count() == 0:\n",
    "    vector_store.add_calls(processed_calls)\n",
    "else:\n",
    "    print(f\"â„¹ï¸ Already {vector_store.collection.count()} documents indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM Analysis Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IVRPatternAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes failure patterns using RAG + local LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: IVRVectorStore, llm_model: str):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm_model = llm_model\n",
    "    \n",
    "    def _build_analysis_prompt(self, new_call: Dict, similar_cases: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Build the analysis prompt with similar cases context.\n",
    "        \"\"\"\n",
    "        # Format similar cases\n",
    "        similar_context = \"\"\n",
    "        if similar_cases['documents'] and similar_cases['documents'][0]:\n",
    "            for i, (doc, meta, dist) in enumerate(zip(\n",
    "                similar_cases['documents'][0],\n",
    "                similar_cases['metadatas'][0],\n",
    "                similar_cases['distances'][0]\n",
    "            ), 1):\n",
    "                similar_context += f\"\\n--- Similar Case #{i} (similarity: {1-dist:.2f}) ---\\n\"\n",
    "                similar_context += f\"Result: {meta.get('final_result', 'N/A')}\\n\"\n",
    "                if meta.get('failure_step'):\n",
    "                    similar_context += f\"Failure step: {meta['failure_step']}\\n\"\n",
    "                    similar_context += f\"Reason: {meta.get('failure_reason', 'N/A')}\\n\"\n",
    "                similar_context += \"\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert in IVR (Interactive Voice Response) system analysis and failure patterns in self-service phone calls.\n",
    "\n",
    "Your task is to analyze a new call and explain the probable cause of failure based on:\n",
    "1. The current call data\n",
    "2. Patterns found in similar historical cases\n",
    "\n",
    "## NEW CALL TO ANALYZE:\n",
    "{new_call['narrative']}\n",
    "\n",
    "## SIMILAR HISTORICAL CASES:\n",
    "{similar_context if similar_context else 'No similar cases found.'}\n",
    "\n",
    "## INSTRUCTIONS:\n",
    "Provide a structured analysis that includes:\n",
    "\n",
    "1. **DIAGNOSIS**: Explain what happened in this call and why it failed.\n",
    "\n",
    "2. **IDENTIFIED PATTERN**: Describe if this failure follows a common pattern based on similar cases.\n",
    "\n",
    "3. **PROBABLE ROOT CAUSE**: Identify the most likely cause of failure.\n",
    "\n",
    "4. **RECOMMENDATIONS**: Suggest specific improvements to prevent this type of failure.\n",
    "\n",
    "5. **CONFIDENCE LEVEL**: Indicate how confident you are in the diagnosis (High/Medium/Low) and why.\n",
    "\n",
    "Be specific with step names and errors.\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def analyze_call(self, call_data: Dict, find_similar: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Analyze a call and generate failure explanation.\n",
    "        \"\"\"\n",
    "        # Process the new call\n",
    "        processor = CallDataProcessor(pd.DataFrame([call_data]))\n",
    "        processed = processor.process_all()[0]\n",
    "        \n",
    "        # Search for similar cases\n",
    "        similar_cases = {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
    "        if find_similar:\n",
    "            similar_cases = self.vector_store.search_similar(\n",
    "                processed['narrative'],\n",
    "                n_results=config.TOP_K_SIMILAR,\n",
    "                filter_result=\"error\"  # Only search among failures\n",
    "            )\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = self._build_analysis_prompt(processed, similar_cases)\n",
    "        \n",
    "        # Generate analysis with LLM\n",
    "        response = ollama.generate(\n",
    "            model=self.llm_model,\n",
    "            prompt=prompt,\n",
    "            options={\n",
    "                \"temperature\": 0.3,  # More deterministic for analysis\n",
    "                \"num_predict\": 1000\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['response']\n",
    "    \n",
    "    def get_pattern_summary(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate a failure pattern summary using the LLM.\n",
    "        \"\"\"\n",
    "        patterns = self.vector_store.get_failure_patterns()\n",
    "        \n",
    "        prompt = f\"\"\"Analyze the following IVR system failure statistics and provide an executive summary:\n",
    "\n",
    "## FAILURE STATISTICS:\n",
    "- Total failed calls: {patterns['total_failures']}\n",
    "\n",
    "### Failures by step:\n",
    "{json.dumps(patterns['by_step'], indent=2)}\n",
    "\n",
    "### Failures by reason:\n",
    "{json.dumps(patterns['by_reason'], indent=2)}\n",
    "\n",
    "### User's final action:\n",
    "{json.dumps(patterns['by_end_action'], indent=2)}\n",
    "\n",
    "Provide:\n",
    "1. The 3 most critical issues\n",
    "2. Concerning patterns\n",
    "3. Prioritized improvement recommendations\n",
    "\n",
    "Respond concisely and actionably.\"\"\"\n",
    "        \n",
    "        response = ollama.generate(\n",
    "            model=self.llm_model,\n",
    "            prompt=prompt,\n",
    "            options={\"temperature\": 0.3}\n",
    "        )\n",
    "        \n",
    "        return response['response']\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = IVRPatternAnalyzer(vector_store, config.LLM_MODEL)\n",
    "print(\"âœ… Pattern analyzer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. System Usage - Practical Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View pattern statistics\n",
    "patterns = vector_store.get_failure_patterns()\n",
    "print(\"ðŸ“Š FAILURE PATTERN STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTotal failed calls: {patterns['total_failures']}\")\n",
    "print(f\"\\nFailures by step:\")\n",
    "for step, count in patterns['by_step'].items():\n",
    "    print(f\"   {step}: {count}\")\n",
    "print(f\"\\nFailures by reason:\")\n",
    "for reason, count in patterns['by_reason'].items():\n",
    "    print(f\"   {reason}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary with LLM\n",
    "print(\"ðŸ“‹ EXECUTIVE PATTERN SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "summary = analyzer.get_pattern_summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Analyze a New Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a new failed call to analyze\n",
    "new_call = {\n",
    "    \"call_id\": \"NEW_001\",\n",
    "    \"timestamp\": pd.Timestamp.now(),\n",
    "    \"steps_completed\": json.dumps([\n",
    "        \"welcome_message\",\n",
    "        \"language_selection\",\n",
    "        \"authentication\"\n",
    "    ]),\n",
    "    \"step_results\": json.dumps({\n",
    "        \"welcome_message\": \"success\",\n",
    "        \"language_selection\": \"success\",\n",
    "        \"authentication\": \"error\"\n",
    "    }),\n",
    "    \"final_result\": \"error\",\n",
    "    \"failure_step\": \"authentication\",\n",
    "    \"failure_reason\": \"auth_timeout\",\n",
    "    \"end_action\": \"transfer_agent\",\n",
    "    \"call_duration_seconds\": 45,\n",
    "    \"retry_count\": 2,\n",
    "    \"customer_segment\": \"premium\",\n",
    "    \"call_type\": \"billing\",\n",
    "    \"hour_of_day\": 14,\n",
    "    \"day_of_week\": \"wednesday\"\n",
    "}\n",
    "\n",
    "print(\"ðŸ” ANALYZING NEW CALL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ID: {new_call['call_id']}\")\n",
    "print(f\"Type: {new_call['call_type']}\")\n",
    "print(f\"Failed at: {new_call['failure_step']}\")\n",
    "print(f\"Reason: {new_call['failure_reason']}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"LLM ANALYSIS:\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "analysis = analyzer.analyze_call(new_call)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Search for Similar Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for cases similar to a specific pattern\n",
    "query = \"billing call that fails at authentication with timeout\"\n",
    "\n",
    "print(f\"ðŸ”Ž Searching for cases similar to: '{query}'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = vector_store.search_similar(\n",
    "    query_text=query,\n",
    "    n_results=3,\n",
    "    filter_result=\"error\"\n",
    ")\n",
    "\n",
    "for i, (doc, meta, dist) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"\\n--- Result #{i} (distance: {dist:.4f}) ---\")\n",
    "    print(f\"Call ID: {meta['call_id']}\")\n",
    "    print(f\"Failure step: {meta.get('failure_step', 'N/A')}\")\n",
    "    print(f\"Reason: {meta.get('failure_reason', 'N/A')}\")\n",
    "    print(f\"Type: {meta.get('call_type', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Analysis Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_failure_analysis(\n",
    "    call_type: str,\n",
    "    failure_step: str,\n",
    "    failure_reason: str,\n",
    "    customer_segment: str = \"standard\",\n",
    "    end_action: str = \"hangup\",\n",
    "    retries: int = 1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Helper function for quick analysis of a failure case.\n",
    "    \n",
    "    Example:\n",
    "        result = quick_failure_analysis(\n",
    "            call_type=\"billing\",\n",
    "            failure_step=\"authentication\",\n",
    "            failure_reason=\"biometric_fail\",\n",
    "            customer_segment=\"premium\"\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Build steps up to failure\n",
    "    all_steps = [\n",
    "        \"welcome_message\",\n",
    "        \"language_selection\",\n",
    "        \"authentication\",\n",
    "        \"main_menu\",\n",
    "        \"inquiry_submenu\",\n",
    "        \"data_entry\",\n",
    "        \"data_validation\",\n",
    "        \"processing\"\n",
    "    ]\n",
    "    \n",
    "    # Find failure step index\n",
    "    if failure_step in all_steps:\n",
    "        fail_idx = all_steps.index(failure_step)\n",
    "        steps_completed = all_steps[:fail_idx + 1]\n",
    "    else:\n",
    "        steps_completed = [\"welcome_message\", failure_step]\n",
    "    \n",
    "    # Build results\n",
    "    step_results = {step: \"success\" for step in steps_completed[:-1]}\n",
    "    step_results[failure_step] = \"error\"\n",
    "    \n",
    "    call = {\n",
    "        \"call_id\": \"QUICK_ANALYSIS\",\n",
    "        \"timestamp\": pd.Timestamp.now(),\n",
    "        \"steps_completed\": json.dumps(steps_completed),\n",
    "        \"step_results\": json.dumps(step_results),\n",
    "        \"final_result\": \"error\",\n",
    "        \"failure_step\": failure_step,\n",
    "        \"failure_reason\": failure_reason,\n",
    "        \"end_action\": end_action,\n",
    "        \"call_duration_seconds\": 60,\n",
    "        \"retry_count\": retries,\n",
    "        \"customer_segment\": customer_segment,\n",
    "        \"call_type\": call_type,\n",
    "        \"hour_of_day\": 12,\n",
    "        \"day_of_week\": \"wednesday\"\n",
    "    }\n",
    "    \n",
    "    return analyzer.analyze_call(call)\n",
    "\n",
    "print(\"âœ… quick_failure_analysis() function available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the quick function\n",
    "print(\"ðŸš€ QUICK ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = quick_failure_analysis(\n",
    "    call_type=\"support\",\n",
    "    failure_step=\"data_entry\",\n",
    "    failure_reason=\"speech_not_recognized\",\n",
    "    customer_segment=\"basic\",\n",
    "    end_action=\"transfer_agent\",\n",
    "    retries=3\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sample dataset for reference\n",
    "df.to_csv(\"ivr_calls_sample.csv\", index=False)\n",
    "print(\"âœ… Dataset saved to 'ivr_calls_sample.csv'\")\n",
    "\n",
    "# Save identified patterns\n",
    "patterns = vector_store.get_failure_patterns()\n",
    "with open(\"failure_patterns.json\", \"w\") as f:\n",
    "    json.dump(patterns, f, indent=2)\n",
    "print(\"âœ… Patterns saved to 'failure_patterns.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Usage Summary\n",
    "\n",
    "### Main Components:\n",
    "\n",
    "1. **`CallDataProcessor`**: Converts CSV data into textual narratives\n",
    "2. **`OllamaEmbeddings`**: Generates embeddings using local model\n",
    "3. **`IVRVectorStore`**: Stores and searches in ChromaDB\n",
    "4. **`IVRPatternAnalyzer`**: Analyzes patterns with LLM\n",
    "\n",
    "### Typical Flow:\n",
    "\n",
    "```python\n",
    "# 1. Load data\n",
    "df = pd.read_csv(\"your_file.csv\")\n",
    "\n",
    "# 2. Process and index\n",
    "processor = CallDataProcessor(df)\n",
    "processed = processor.process_all()\n",
    "vector_store.add_calls(processed)\n",
    "\n",
    "# 3. Analyze new call\n",
    "result = analyzer.analyze_call(new_call)\n",
    "\n",
    "# 4. Or use quick function\n",
    "result = quick_failure_analysis(\n",
    "    call_type=\"billing\",\n",
    "    failure_step=\"authentication\",\n",
    "    failure_reason=\"timeout\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Recommended Ollama Models:\n",
    "\n",
    "- **Embeddings**: `nomic-embed-text` (fast and effective)\n",
    "- **LLM**: `llama3.1` or `mistral` (good for analysis)\n",
    "- **Lightweight alternative**: `phi3` or `gemma2` if you have limited resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
