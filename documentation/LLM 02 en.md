Este es un excelente caso de uso para una arquitectura **RAG (Retrieval-Augmented Generation)** local. Dado que tienes restricciones estrictas (sin nube, solo local, acceso a una base de datos vectorial), no necesitas "entrenar" (fine-tune) el modelo modificando sus pesos, lo cual es costoso y lento.

En su lugar, utilizar치s la **Base de Datos Vectorial como la "memoria a largo plazo"** de patrones pasados. El flujo l칩gico ser치:

1.  **Indexar:** Convertir tus logs hist칩ricos (CSV) en vectores y guardarlos.
2.  **Recuperar:** Cuando llega un nuevo caso, buscar los casos hist칩ricos m치s similares (donde ya sabes qu칠 pas칩).
3.  **Inferir:** Pasarle al LLM (Ollama) el nuevo caso + los casos similares recuperados y pedirle que detecte el patr칩n de falla.

Aqu칤 tienes la gu칤a paso a paso y el c칩digo necesario para tu Notebook.

### Prerrequisitos

Necesitar치s instalar las siguientes librer칤as en tu entorno local:

```bash
pip install pandas chromadb sentence-transformers ollama
```

  * **Pandas:** Manipulaci칩n del CSV.
  * **ChromaDB:** Tu base de datos vectorial local (ligera y no requiere servidor).
  * **Sentence-transformers:** Para crear los "embeddings" (convertir texto a n칰meros) localmente sin depender del LLM para esto (es m치s r치pido).
  * **Ollama:** Para interactuar con tu modelo local (Llama 3, Mistral, etc.).

-----

### Paso 1: Preparaci칩n y Carga de Datos (CSV)

Primero, debemos transformar las filas de tu CSV en un formato narrativo que el modelo pueda entender sem치nticamente.

```python
import pandas as pd

# 1. Cargar el CSV
# Supongamos que tu CSV tiene: call_id, steps_history, final_status, error_code, customer_segment
df = pd.read_csv('call_center_data.csv')

# 2. Crear una columna de "Contexto"
# Convertimos la fila estructurada en un texto descriptivo.
# Esto ayuda al modelo a encontrar patrones sem치nticos.
def create_context(row):
    return f"""
    Interacci칩n ID: {row['call_id']}.
    Secuencia de pasos: {row['steps_history']}.
    Estado Final: {row['final_status']}.
    C칩digo de Error: {row['error_code']}.
    Nota: {row.get('notes', 'Sin notas adicionales')}
    """

df['text_for_embedding'] = df.apply(create_context, axis=1)

# Separar datos: 칄xitos vs Errores (para que el modelo compare)
# Aunque para la DB vectorial es mejor meter todo para tener contexto completo.
documents = df['text_for_embedding'].tolist()
ids = df['call_id'].astype(str).tolist()
metadatas = df[['final_status', 'error_code']].to_dict(orient='records')

print(f"Datos preparados: {len(documents)} registros.")
```

-----

### Paso 2: Crear la Base de Datos Vectorial Local

Aqu칤 es donde "guardamos los patrones". Usaremos `sentence-transformers` para crear embeddings de alta calidad localmente y `ChromaDB` para guardarlos.

```python
import chromadb
from chromadb.utils import embedding_functions

# 1. Configurar el cliente local de ChromaDB (se guarda en una carpeta local)
chroma_client = chromadb.PersistentClient(path="./my_local_vectordb")

# 2. Configurar la funci칩n de embedding local
# 'all-MiniLM-L6-v2' es peque침o, r치pido y muy bueno para clustering sem치ntico.
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="all-MiniLM-L6-v2" 
)

# 3. Crear o conectar a la colecci칩n
collection = chroma_client.get_or_create_collection(
    name="call_patterns",
    embedding_function=sentence_transformer_ef
)

# 4. Inyectar la data (Esto se hace una sola vez o incrementalmente)
# Chroma maneja la tokenizaci칩n y vectorizaci칩n autom치ticamente con la funci칩n definida arriba.
collection.upsert(
    documents=documents,
    ids=ids,
    metadatas=metadatas
)

print("Base de datos vectorial actualizada localmente.")
```

-----

### Paso 3: L칩gica de Detecci칩n de Patrones (El "Cerebro")

Ahora creamos la funci칩n que usa **Ollama**. Esta funci칩n toma un "nuevo caso" (ej. una llamada que fall칩 hoy), busca en la DB vectorial qu칠 pas칩 en casos parecidos anteriormente, y le pide a Ollama que diagnostique.

```python
import ollama

def analizar_incidente(nuevo_caso_texto):
    
    # 1. B칔SQUEDA SEM츼NTICA (RAG)
    # Buscamos los 5 casos hist칩ricos m치s parecidos a este nuevo problema
    results = collection.query(
        query_texts=[nuevo_caso_texto],
        n_results=5
    )
    
    contexto_recuperado = "\n".join(results['documents'][0])
    
    # 2. CONSTRUCCI칍N DEL PROMPT
    # Le damos al LLM el nuevo caso + la "memoria" de casos similares.
    prompt = f"""
    Eres un analista experto en IVR y experiencia de usuario. 
    
    Tu objetivo es identificar la CAUSA RA칈Z de un fallo en una interacci칩n reciente bas치ndote en patrones hist칩ricos.
    
    --- INFORMACI칍N HIST칍RICA SIMILAR (Base de Conocimiento) ---
    {contexto_recuperado}
    ------------------------------------------------------------
    
    --- NUEVO CASO A ANALIZAR ---
    {nuevo_caso_texto}
    -----------------------------
    
    INSTRUCCIONES:
    1. Compara el 'NUEVO CASO' con la 'INFORMACI칍N HIST칍RICA'.
    2. Identifica si existe un patr칩n com칰n en los pasos previos al error (ej. siempre falla despu칠s del paso X).
    3. Explica por qu칠 probablemente fall칩 este caso (ej. Hangup por frustraci칩n, error t칠cnico, transferencia forzada).
    4. S칠 conciso y t칠cnico.
    
    AN츼LISIS:
    """
    
    # 3. LLAMADA A OLLAMA (Local)
    # Aseg칰rate de tener corriendo 'ollama serve' y haber hecho 'ollama pull llama3' (o mistral)
    response = ollama.chat(model='llama3', messages=[
        {'role': 'user', 'content': prompt},
    ])
    
    return response['message']['content']

```

-----

### Paso 4: Ejecuci칩n (Probando el sistema)

Supongamos que tienes una nueva llamada que fall칩 y quieres saber por qu칠, basado en lo que el sistema "aprendi칩" (index칩) de tu CSV.

```python
# Ejemplo de un caso nuevo que entra hoy
nuevo_caso = """
Interacci칩n ID: NEW_999.
Secuencia de pasos: Inicio -> Autenticaci칩n -> Consulta Saldo -> Men칰 Pagos -> ERROR_API_TIMEOUT.
Estado Final: Error.
Nota: Cliente colg칩 despu칠s de 5 segundos de silencio.
"""

# Ejecutamos el an치lisis
analisis = analizar_incidente(nuevo_caso)

print("### Resultado del An치lisis de Patrones ###")
print(analisis)
```

### 쯇or qu칠 esta arquitectura es la mejor para tu caso?

1.  **Detecci칩n de Patrones Reales:** Si en tu CSV hist칩rico hay 50 llamadas que fallaron en "Men칰 Pagos" por un error de API, la base de datos vectorial traer치 esos ejemplos espec칤ficos. Ollama leer치 esos ejemplos y dir치: *"Basado en los registros hist칩ricos, este fallo coincide con un patr칩n recurrente de Timeouts en el m칩dulo de Pagos, lo que usualmente resulta en abandono (hang up) por parte del usuario debido a la espera."*
2.  **100% Local:** ChromaDB guarda archivos en disco. Ollama corre en tu CPU/GPU. Nada sale de tu m치quina.
3.  **Sin "Entrenamiento" pesado:** No necesitas gastar d칤as entrenando un modelo. Si ma침ana tienes un nuevo CSV con m치s datos, solo haces `collection.upsert` y el modelo es instant치neamente m치s inteligente.

### Resumen de pasos para ti:

1.  Limpia tu CSV para que los nombres de los pasos sean consistentes.
2.  Corre el script de **Carga** (Paso 1 y 2) una sola vez para llenar la base de datos.
3.  Usa el script de **An치lisis** (Paso 3) cada vez que quieras diagnosticar una falla.

Aqu칤 tienes el c칩digo completo estructurado para copiar y pegar directamente en un **Jupyter Notebook**.

Lo he dise침ado para que sea **autocontenido**: incluye un generador de datos falsos (para que puedas probarlo ahora mismo sin tu CSV real) y luego la l칩gica para cargar tu CSV real.

### Requisitos previos (Terminal)

Antes de abrir el notebook, aseg칰rate de tener **Ollama** corriendo en tu m치quina con el modelo que quieras usar (ej. Llama3):

Bash

```
ollama serve
ollama pull llama3
```

---

### 游닂 TU NOTEBOOK: Detecci칩n de Patrones en IVR con RAG Local

Copia cada bloque de c칩digo en una celda separada de tu Jupyter Notebook.

#### Celda 1: Instalaci칩n de Librer칤as

Instalamos las dependencias necesarias. `chromadb` es nuestra base de datos vectorial local y `sentence-transformers` genera los vectores num칠ricos sin depender de la nube.

Python

```
!pip install pandas chromadb sentence-transformers ollama
```

#### Celda 2: Importaciones y Configuraci칩n

Configuramos las librer칤as.

Python

```
import pandas as pd
import chromadb
from chromadb.utils import embedding_functions
import ollama
import os

# Configuraci칩n
MODELO_LLM = "llama3"  # Aseg칰rate de tenerlo descargado en Ollama
COLECCION_NOMBRE = "ivr_patterns_db"
PATH_DB_VECTORIAL = "./local_chroma_db" # Carpeta donde se guardar치 la "memoria"

print("Librer칤as importadas correctamente.")
```

#### Celda 3: Carga de Datos (Opci칩n A: Generar datos de prueba)

Si a칰n no tienes tu CSV limpio, ejecuta esta celda para crear un dataset de prueba que simula logs de IVR con errores y 칠xitos.

Python

```
# --- GENERADOR DE DATOS DE PRUEBA (SOLO SI NO TIENES CSV A칔N) ---
data = {
    'call_id': [101, 102, 103, 104, 105, 106],
    'steps_history': [
        "Inicio -> Menu Principal -> Consulta Saldo -> Fin",
        "Inicio -> Menu Principal -> Pagos -> Ingreso Tarjeta -> Error 500",
        "Inicio -> Menu Principal -> Pagos -> Ingreso Tarjeta -> Timeout",
        "Inicio -> Soporte -> Espera -> Agente",
        "Inicio -> Menu Principal -> Pagos -> Ingreso Tarjeta -> Rechazada",
        "Inicio -> Menu Principal -> Pagos -> Ingreso Tarjeta -> Error 503"
    ],
    'final_status': ['Exito', 'Error', 'Hangup', 'Transferencia', 'Error', 'Error'],
    'error_code': ['N/A', 'API_FAIL', 'USER_TIMEOUT', 'N/A', 'BANK_DECLINE', 'API_FAIL'],
    'user_segment': ['Gold', 'Standard', 'Standard', 'Gold', 'Premium', 'Standard']
}

df = pd.DataFrame(data)
print("Datos de prueba generados:")
display(df.head())
```

#### Celda 4: Carga de Datos (Opci칩n B: Tu CSV Real)

Cuando tengas tu archivo, usa esta celda en lugar de la anterior.

Python

```
# Descomenta las l칤neas de abajo para usar tu archivo real
# df = pd.read_csv('tu_archivo_de_datos.csv')

# Aseg칰rate de que las columnas coincidan o ren칩mbralas
# df = df.rename(columns={'id_llamada': 'call_id', ...})
```

#### Celda 5: Preparaci칩n de Datos y Creaci칩n de Embeddings

Aqu칤 convertimos las filas de la tabla en "Historias" de texto para que el modelo entienda el contexto sem치ntico (ej: que un Error 503 es similar a un Error 500).

Python

```
# 1. Funci칩n para crear una narrativa de texto por fila
def row_to_text(row):
    return f"""
    ID Interacci칩n: {row['call_id']}
    Secuencia de Pasos: {row['steps_history']}
    Resultado Final: {row['final_status']}
    C칩digo de Error: {row['error_code']}
    Segmento Usuario: {row.get('user_segment', 'General')}
    """

# Aplicamos la funci칩n
df['text_content'] = df.apply(row_to_text, axis=1)

# 2. Inicializar ChromaDB (Base de datos vectorial local)
client = chromadb.PersistentClient(path=PATH_DB_VECTORIAL)

# Usamos un modelo de embeddings ligero y local (muy r치pido)
# all-MiniLM-L6-v2 es est치ndar para esto.
emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="all-MiniLM-L6-v2"
)

# 3. Crear o resetear la colecci칩n
try:
    client.delete_collection(name=COLECCION_NOMBRE) # Limpiamos si ya exist칤a para evitar duplicados en pruebas
except:
    pass

collection = client.create_collection(
    name=COLECCION_NOMBRE,
    embedding_function=emb_fn
)

# 4. Inyectar datos en la DB Vectorial
print("Indexando datos en la base vectorial local...")
collection.add(
    documents=df['text_content'].tolist(),
    metadatas=df[['final_status', 'error_code']].to_dict(orient='records'),
    ids=[str(x) for x in df['call_id'].tolist()]
)

print(f"춰칄xito! {len(df)} registros indexados en la memoria local.")
```

#### Celda 6: La L칩gica de IA (El "Cerebro")

Esta funci칩n hace la magia. Recibe un caso nuevo, busca en la DB casos parecidos, y se los env칤a a Ollama para que opine.

Python

```
def analizar_patron_falla(nuevo_caso_steps, nuevo_caso_error):
    
    # 1. Construir el texto de b칰squeda
    query_text = f"Secuencia: {nuevo_caso_steps}. Error: {nuevo_caso_error}"
    
    # 2. Recuperar contextos similares (RAG)
    results = collection.query(
        query_texts=[query_text],
        n_results=3  # Traemos los 3 casos hist칩ricos m치s parecidos
    )
    
    contexto = "\n---\n".join(results['documents'][0])
    
    # 3. Prompt para Ollama
    prompt = f"""
    Act칰a como un analista experto en datos de Call Center e IVR.
    Tu tarea es analizar por qu칠 fall칩 una interacci칩n reciente bas치ndote en patrones hist칩ricos.

    === HISTORIAL DE CASOS SIMILARES (MEMORIA) ===
    {contexto}
    ==============================================

    === NUEVO CASO A ANALIZAR ===
    Pasos: {nuevo_caso_steps}
    Error Reportado: {nuevo_caso_error}
    =============================

    INSTRUCCIONES:
    1. Analiza los "Casos Similares". 쯌es un patr칩n en donde ocurren los fallos?
    2. Compara con el "Nuevo Caso".
    3. Predice la causa ra칤z m치s probable (ej: fallo de API en paso de pago, usuario frustrado por longitud del men칰, etc.).
    4. Responde en Espa침ol, directo y conciso.
    """

    # 4. Llamada al modelo local
    print("Consultando a Ollama (esto puede tardar unos segundos dependiendo de tu CPU/GPU)...")
    response = ollama.chat(model=MODELO_LLM, messages=[
        {'role': 'user', 'content': prompt},
    ])
    
    return response['message']['content']
```

#### Celda 7: Ejecuci칩n y Prueba

Simulamos que entra una llamada nueva que fall칩 y le preguntamos al sistema.

Python

```
# --- SIMULACI칍N DE UN CASO NUEVO ---
# Imagina que esto acaba de pasar en producci칩n:
nueva_secuencia = "Inicio -> Menu Principal -> Pagos -> Ingreso Tarjeta -> Error de conexi칩n"
nuevo_error = "API_TIMEOUT"

print(f"Analizando incidente: {nuevo_error} en {nueva_secuencia}\n")

resultado = analizar_patron_falla(nueva_secuencia, nuevo_error)

print("-" * 30)
print("REPORTE DEL MODELO:")
print(resultado)
print("-" * 30)
```

### 쯈u칠 est치 pasando "por detr치s"?

1. **ChromaDB** convierte tu texto "Ingreso Tarjeta -> Error de conexi칩n" en n칰meros.
    
2. Busca en los vectores guardados y encuentra que los IDs 102 y 106 (del generador de datos) son matem치ticamente muy cercanos porque tambi칠n tuvieron problemas en "Pagos" e "Ingreso Tarjeta".
    
3. Recupera esos textos completos.
    
4. **Ollama** recibe un prompt que dice: _"Mira, en el pasado, cuando la gente entraba a Pagos y fallaba, era usualmente un 'API_FAIL'. Ahora tengo este caso nuevo. 쯈u칠 opinas?"_
    
5. Ollama razona y te responde identificando el patr칩n.
    

Esta estructura es totalmente local, privada y no requiere internet para funcionar una vez instaladas las librer칤as.