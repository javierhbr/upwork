{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìû An√°lisis de Patrones en Llamadas IVR con LLM Local\n",
    "\n",
    "Este notebook implementa un sistema completo para:\n",
    "1. Cargar y procesar datos de llamadas telef√≥nicas self-service\n",
    "2. Generar embeddings locales usando Ollama\n",
    "3. Almacenar en ChromaDB (base vectorial local)\n",
    "4. Detectar patrones de fallo y explicarlos con LLM\n",
    "\n",
    "**Requisitos:**\n",
    "- Ollama instalado y corriendo localmente\n",
    "- Modelo descargado (ej: `ollama pull llama3.1` y `ollama pull nomic-embed-text`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias necesarias\n",
    "!pip install chromadb pandas ollama scikit-learn matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import ollama\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de Ollama y ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuraci√≥n centralizada del sistema\"\"\"\n",
    "    # Modelos de Ollama\n",
    "    EMBEDDING_MODEL: str = \"nomic-embed-text\"  # Modelo para embeddings\n",
    "    LLM_MODEL: str = \"llama3.1\"                # Modelo para an√°lisis/explicaciones\n",
    "    \n",
    "    # ChromaDB\n",
    "    CHROMA_PERSIST_DIR: str = \"./chroma_ivr_db\"\n",
    "    COLLECTION_NAME: str = \"ivr_call_patterns\"\n",
    "    \n",
    "    # An√°lisis\n",
    "    TOP_K_SIMILAR: int = 5  # N√∫mero de casos similares a recuperar\n",
    "    \n",
    "config = Config()\n",
    "print(f\"üìã Configuraci√≥n:\")\n",
    "print(f\"   - Modelo embeddings: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"   - Modelo LLM: {config.LLM_MODEL}\")\n",
    "print(f\"   - Directorio ChromaDB: {config.CHROMA_PERSIST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar conexi√≥n con Ollama\n",
    "def verify_ollama_connection():\n",
    "    \"\"\"Verifica que Ollama est√© corriendo y los modelos disponibles\"\"\"\n",
    "    try:\n",
    "        models = ollama.list()\n",
    "        model_names = [m['name'].split(':')[0] for m in models.get('models', [])]\n",
    "        print(\"‚úÖ Ollama conectado correctamente\")\n",
    "        print(f\"   Modelos disponibles: {model_names}\")\n",
    "        \n",
    "        # Verificar modelos requeridos\n",
    "        required = [config.EMBEDDING_MODEL, config.LLM_MODEL]\n",
    "        for model in required:\n",
    "            if model not in model_names:\n",
    "                print(f\"‚ö†Ô∏è  Modelo '{model}' no encontrado. Ejecuta: ollama pull {model}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error conectando con Ollama: {e}\")\n",
    "        print(\"   Aseg√∫rate de que Ollama est√© corriendo: ollama serve\")\n",
    "        return False\n",
    "\n",
    "verify_ollama_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generaci√≥n de Dataset de Ejemplo\n",
    "\n",
    "Si ya tienes tu CSV, salta esta celda y carga tu archivo directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_dataset(n_calls: int = 500) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Genera un dataset de ejemplo simulando llamadas IVR.\n",
    "    Reemplaza esto con tu CSV real.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Definir pasos del flujo IVR\n",
    "    ivr_steps = [\n",
    "        \"welcome_message\",\n",
    "        \"language_selection\",\n",
    "        \"authentication\",\n",
    "        \"menu_principal\",\n",
    "        \"submenu_consultas\",\n",
    "        \"ingreso_datos\",\n",
    "        \"validacion_datos\",\n",
    "        \"procesamiento\",\n",
    "        \"confirmacion\",\n",
    "        \"despedida\"\n",
    "    ]\n",
    "    \n",
    "    # Posibles errores por paso\n",
    "    error_types = {\n",
    "        \"authentication\": [\"auth_timeout\", \"invalid_credentials\", \"account_locked\", \"biometric_fail\"],\n",
    "        \"ingreso_datos\": [\"dtmf_timeout\", \"invalid_input\", \"speech_not_recognized\", \"too_many_retries\"],\n",
    "        \"validacion_datos\": [\"data_mismatch\", \"expired_data\", \"system_unavailable\"],\n",
    "        \"procesamiento\": [\"backend_timeout\", \"service_unavailable\", \"transaction_failed\"],\n",
    "        \"menu_principal\": [\"no_input\", \"invalid_option\", \"confusion_detected\"]\n",
    "    }\n",
    "    \n",
    "    calls = []\n",
    "    \n",
    "    for i in range(n_calls):\n",
    "        call_id = f\"CALL_{i:05d}\"\n",
    "        \n",
    "        # Determinar si la llamada ser√° exitosa (60% √©xito)\n",
    "        is_success = np.random.random() < 0.6\n",
    "        \n",
    "        if is_success:\n",
    "            # Llamada exitosa - completa todos los pasos\n",
    "            steps_completed = ivr_steps.copy()\n",
    "            step_results = {step: \"success\" for step in steps_completed}\n",
    "            final_result = \"success\"\n",
    "            failure_step = None\n",
    "            failure_reason = None\n",
    "            end_action = \"completed\"\n",
    "        else:\n",
    "            # Llamada fallida - falla en alg√∫n paso\n",
    "            fail_step_idx = np.random.choice([2, 3, 4, 5, 6, 7], p=[0.25, 0.1, 0.1, 0.25, 0.15, 0.15])\n",
    "            steps_completed = ivr_steps[:fail_step_idx + 1]\n",
    "            \n",
    "            step_results = {step: \"success\" for step in steps_completed[:-1]}\n",
    "            failure_step = steps_completed[-1]\n",
    "            \n",
    "            # Asignar tipo de error\n",
    "            if failure_step in error_types:\n",
    "                failure_reason = np.random.choice(error_types[failure_step])\n",
    "            else:\n",
    "                failure_reason = \"unknown_error\"\n",
    "            \n",
    "            step_results[failure_step] = \"error\"\n",
    "            final_result = \"error\"\n",
    "            \n",
    "            # Acci√≥n final\n",
    "            end_action = np.random.choice([\"hangup\", \"transfer_agent\"], p=[0.4, 0.6])\n",
    "        \n",
    "        # Metadata adicional\n",
    "        call_duration = np.random.randint(30, 300) if is_success else np.random.randint(15, 180)\n",
    "        retries = 0 if is_success else np.random.randint(1, 4)\n",
    "        \n",
    "        calls.append({\n",
    "            \"call_id\": call_id,\n",
    "            \"timestamp\": pd.Timestamp.now() - pd.Timedelta(days=np.random.randint(0, 30)),\n",
    "            \"steps_completed\": json.dumps(steps_completed),\n",
    "            \"step_results\": json.dumps(step_results),\n",
    "            \"final_result\": final_result,\n",
    "            \"failure_step\": failure_step,\n",
    "            \"failure_reason\": failure_reason,\n",
    "            \"end_action\": end_action,\n",
    "            \"call_duration_seconds\": call_duration,\n",
    "            \"retry_count\": retries,\n",
    "            \"customer_segment\": np.random.choice([\"premium\", \"standard\", \"basic\"]),\n",
    "            \"call_type\": np.random.choice([\"billing\", \"support\", \"sales\", \"account_info\"]),\n",
    "            \"hour_of_day\": np.random.randint(8, 22),\n",
    "            \"day_of_week\": np.random.choice([\"lunes\", \"martes\", \"miercoles\", \"jueves\", \"viernes\"])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(calls)\n",
    "\n",
    "# Generar dataset de ejemplo\n",
    "df = generate_sample_dataset(500)\n",
    "print(f\"üìä Dataset generado: {len(df)} llamadas\")\n",
    "print(f\"   - Exitosas: {len(df[df['final_result'] == 'success'])}\")\n",
    "print(f\"   - Fallidas: {len(df[df['final_result'] == 'error'])}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚¨áÔ∏è ALTERNATIVA: Cargar tu propio CSV\n",
    "# Descomenta y ajusta seg√∫n tu estructura de datos\n",
    "\n",
    "# df = pd.read_csv(\"tu_archivo.csv\")\n",
    "# print(f\"Dataset cargado: {len(df)} registros\")\n",
    "# print(f\"Columnas: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento y Creaci√≥n de Representaciones Textuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallDataProcessor:\n",
    "    \"\"\"\n",
    "    Procesa los datos de llamadas y genera representaciones textuales\n",
    "    para crear embeddings significativos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.processed_calls = []\n",
    "    \n",
    "    def create_call_narrative(self, row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Convierte una fila del DataFrame en una narrativa textual\n",
    "        que capture el contexto completo de la llamada.\n",
    "        \"\"\"\n",
    "        # Parsear JSON si es necesario\n",
    "        if isinstance(row['steps_completed'], str):\n",
    "            steps = json.loads(row['steps_completed'])\n",
    "        else:\n",
    "            steps = row['steps_completed']\n",
    "            \n",
    "        if isinstance(row['step_results'], str):\n",
    "            results = json.loads(row['step_results'])\n",
    "        else:\n",
    "            results = row['step_results']\n",
    "        \n",
    "        # Construir narrativa\n",
    "        narrative_parts = [\n",
    "            f\"Llamada tipo: {row['call_type']}\",\n",
    "            f\"Segmento cliente: {row['customer_segment']}\",\n",
    "            f\"D√≠a: {row['day_of_week']}, Hora: {row['hour_of_day']}:00\",\n",
    "            f\"Duraci√≥n: {row['call_duration_seconds']} segundos\",\n",
    "            \"\\nFlujo de pasos:\"\n",
    "        ]\n",
    "        \n",
    "        # Describir cada paso\n",
    "        for i, step in enumerate(steps, 1):\n",
    "            result = results.get(step, 'unknown')\n",
    "            status = \"‚úì\" if result == \"success\" else \"‚úó\"\n",
    "            narrative_parts.append(f\"  {i}. {step}: {status} ({result})\")\n",
    "        \n",
    "        # Resultado final\n",
    "        narrative_parts.append(f\"\\nResultado final: {row['final_result'].upper()}\")\n",
    "        \n",
    "        if row['final_result'] == 'error':\n",
    "            narrative_parts.extend([\n",
    "                f\"Paso de fallo: {row['failure_step']}\",\n",
    "                f\"Raz√≥n del fallo: {row['failure_reason']}\",\n",
    "                f\"Acci√≥n final: {row['end_action']}\",\n",
    "                f\"Intentos de reintento: {row['retry_count']}\"\n",
    "            ])\n",
    "        \n",
    "        return \"\\n\".join(narrative_parts)\n",
    "    \n",
    "    def create_pattern_signature(self, row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Crea una firma de patr√≥n m√°s concisa para b√∫squeda r√°pida.\n",
    "        \"\"\"\n",
    "        if isinstance(row['steps_completed'], str):\n",
    "            steps = json.loads(row['steps_completed'])\n",
    "        else:\n",
    "            steps = row['steps_completed']\n",
    "        \n",
    "        signature_parts = [\n",
    "            f\"result:{row['final_result']}\",\n",
    "            f\"steps:{len(steps)}\",\n",
    "            f\"type:{row['call_type']}\",\n",
    "            f\"segment:{row['customer_segment']}\"\n",
    "        ]\n",
    "        \n",
    "        if row['final_result'] == 'error':\n",
    "            signature_parts.extend([\n",
    "                f\"fail_step:{row['failure_step']}\",\n",
    "                f\"fail_reason:{row['failure_reason']}\",\n",
    "                f\"end_action:{row['end_action']}\"\n",
    "            ])\n",
    "        \n",
    "        return \" | \".join(signature_parts)\n",
    "    \n",
    "    def process_all(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Procesa todas las llamadas y retorna lista de documentos.\n",
    "        \"\"\"\n",
    "        self.processed_calls = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            doc = {\n",
    "                \"id\": row['call_id'],\n",
    "                \"narrative\": self.create_call_narrative(row),\n",
    "                \"signature\": self.create_pattern_signature(row),\n",
    "                \"metadata\": {\n",
    "                    \"call_id\": row['call_id'],\n",
    "                    \"final_result\": row['final_result'],\n",
    "                    \"failure_step\": row['failure_step'] if pd.notna(row['failure_step']) else None,\n",
    "                    \"failure_reason\": row['failure_reason'] if pd.notna(row['failure_reason']) else None,\n",
    "                    \"call_type\": row['call_type'],\n",
    "                    \"customer_segment\": row['customer_segment'],\n",
    "                    \"end_action\": row['end_action']\n",
    "                }\n",
    "            }\n",
    "            self.processed_calls.append(doc)\n",
    "        \n",
    "        return self.processed_calls\n",
    "\n",
    "# Procesar datos\n",
    "processor = CallDataProcessor(df)\n",
    "processed_calls = processor.process_all()\n",
    "\n",
    "print(f\"‚úÖ {len(processed_calls)} llamadas procesadas\")\n",
    "print(\"\\nüìù Ejemplo de narrativa generada:\")\n",
    "print(\"-\" * 50)\n",
    "# Mostrar ejemplo de llamada fallida\n",
    "failed_example = next(c for c in processed_calls if c['metadata']['final_result'] == 'error')\n",
    "print(failed_example['narrative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generaci√≥n de Embeddings con Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaEmbeddings:\n",
    "    \"\"\"\n",
    "    Genera embeddings usando Ollama localmente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"nomic-embed-text\"):\n",
    "        self.model = model\n",
    "    \n",
    "    def embed_single(self, text: str) -> List[float]:\n",
    "        \"\"\"Genera embedding para un texto individual.\"\"\"\n",
    "        response = ollama.embeddings(\n",
    "            model=self.model,\n",
    "            prompt=text\n",
    "        )\n",
    "        return response['embedding']\n",
    "    \n",
    "    def embed_batch(self, texts: List[str], show_progress: bool = True) -> List[List[float]]:\n",
    "        \"\"\"Genera embeddings para m√∫ltiples textos.\"\"\"\n",
    "        embeddings = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            emb = self.embed_single(text)\n",
    "            embeddings.append(emb)\n",
    "            \n",
    "            if show_progress and (i + 1) % 50 == 0:\n",
    "                print(f\"   Procesados: {i + 1}/{total}\")\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Inicializar generador de embeddings\n",
    "embedder = OllamaEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "\n",
    "# Test r√°pido\n",
    "test_emb = embedder.embed_single(\"prueba de embedding\")\n",
    "print(f\"‚úÖ Embeddings funcionando - Dimensi√≥n: {len(test_emb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Almacenamiento en ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IVRVectorStore:\n",
    "    \"\"\"\n",
    "    Gestiona el almacenamiento vectorial de llamadas IVR usando ChromaDB.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, persist_dir: str, collection_name: str, embedder: OllamaEmbeddings):\n",
    "        self.embedder = embedder\n",
    "        \n",
    "        # Inicializar ChromaDB con persistencia\n",
    "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
    "        \n",
    "        # Crear o recuperar colecci√≥n\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"IVR call patterns for failure analysis\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ ChromaDB inicializado\")\n",
    "        print(f\"   Colecci√≥n: {collection_name}\")\n",
    "        print(f\"   Documentos existentes: {self.collection.count()}\")\n",
    "    \n",
    "    def add_calls(self, processed_calls: List[Dict], batch_size: int = 100):\n",
    "        \"\"\"\n",
    "        A√±ade llamadas procesadas a la base vectorial.\n",
    "        \"\"\"\n",
    "        print(f\"\\nüì• Indexando {len(processed_calls)} llamadas...\")\n",
    "        \n",
    "        # Preparar datos para ChromaDB\n",
    "        ids = []\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        \n",
    "        for call in processed_calls:\n",
    "            ids.append(call['id'])\n",
    "            # Combinamos narrativa y firma para el documento\n",
    "            documents.append(f\"{call['narrative']}\\n\\nPatr√≥n: {call['signature']}\")\n",
    "            metadatas.append(call['metadata'])\n",
    "        \n",
    "        # Generar embeddings\n",
    "        print(\"   Generando embeddings...\")\n",
    "        embeddings = self.embedder.embed_batch(documents)\n",
    "        \n",
    "        # Insertar en batches\n",
    "        for i in range(0, len(ids), batch_size):\n",
    "            end_idx = min(i + batch_size, len(ids))\n",
    "            \n",
    "            self.collection.add(\n",
    "                ids=ids[i:end_idx],\n",
    "                documents=documents[i:end_idx],\n",
    "                embeddings=embeddings[i:end_idx],\n",
    "                metadatas=metadatas[i:end_idx]\n",
    "            )\n",
    "        \n",
    "        print(f\"‚úÖ {len(ids)} llamadas indexadas correctamente\")\n",
    "        print(f\"   Total en colecci√≥n: {self.collection.count()}\")\n",
    "    \n",
    "    def search_similar(self, query_text: str, n_results: int = 5, \n",
    "                       filter_result: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Busca llamadas similares a una consulta.\n",
    "        \"\"\"\n",
    "        # Generar embedding de la consulta\n",
    "        query_embedding = self.embedder.embed_single(query_text)\n",
    "        \n",
    "        # Construir filtro opcional\n",
    "        where_filter = None\n",
    "        if filter_result:\n",
    "            where_filter = {\"final_result\": filter_result}\n",
    "        \n",
    "        # Buscar\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            where=where_filter,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_failure_patterns(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Obtiene estad√≠sticas de patrones de fallo.\n",
    "        \"\"\"\n",
    "        # Obtener todos los documentos de error\n",
    "        results = self.collection.get(\n",
    "            where={\"final_result\": \"error\"},\n",
    "            include=[\"metadatas\"]\n",
    "        )\n",
    "        \n",
    "        # Analizar patrones\n",
    "        failure_steps = Counter()\n",
    "        failure_reasons = Counter()\n",
    "        end_actions = Counter()\n",
    "        \n",
    "        for meta in results['metadatas']:\n",
    "            if meta.get('failure_step'):\n",
    "                failure_steps[meta['failure_step']] += 1\n",
    "            if meta.get('failure_reason'):\n",
    "                failure_reasons[meta['failure_reason']] += 1\n",
    "            if meta.get('end_action'):\n",
    "                end_actions[meta['end_action']] += 1\n",
    "        \n",
    "        return {\n",
    "            \"total_failures\": len(results['metadatas']),\n",
    "            \"by_step\": dict(failure_steps.most_common()),\n",
    "            \"by_reason\": dict(failure_reasons.most_common()),\n",
    "            \"by_end_action\": dict(end_actions.most_common())\n",
    "        }\n",
    "    \n",
    "    def clear_collection(self):\n",
    "        \"\"\"Limpia la colecci√≥n (√∫til para re-entrenar).\"\"\"\n",
    "        self.client.delete_collection(self.collection.name)\n",
    "        self.collection = self.client.create_collection(\n",
    "            name=self.collection.name,\n",
    "            metadata={\"description\": \"IVR call patterns for failure analysis\"}\n",
    "        )\n",
    "        print(\"üóëÔ∏è Colecci√≥n limpiada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar vector store\n",
    "vector_store = IVRVectorStore(\n",
    "    persist_dir=config.CHROMA_PERSIST_DIR,\n",
    "    collection_name=config.COLLECTION_NAME,\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "# Limpiar si queremos empezar fresco (opcional)\n",
    "# vector_store.clear_collection()\n",
    "\n",
    "# Indexar llamadas solo si la colecci√≥n est√° vac√≠a\n",
    "if vector_store.collection.count() == 0:\n",
    "    vector_store.add_calls(processed_calls)\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Ya hay {vector_store.collection.count()} documentos indexados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Motor de An√°lisis con LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IVRPatternAnalyzer:\n",
    "    \"\"\"\n",
    "    Analiza patrones de fallo usando RAG + LLM local.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: IVRVectorStore, llm_model: str):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm_model = llm_model\n",
    "    \n",
    "    def _build_analysis_prompt(self, new_call: Dict, similar_cases: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Construye el prompt para el an√°lisis con contexto de casos similares.\n",
    "        \"\"\"\n",
    "        # Formatear casos similares\n",
    "        similar_context = \"\"\n",
    "        if similar_cases['documents'] and similar_cases['documents'][0]:\n",
    "            for i, (doc, meta, dist) in enumerate(zip(\n",
    "                similar_cases['documents'][0],\n",
    "                similar_cases['metadatas'][0],\n",
    "                similar_cases['distances'][0]\n",
    "            ), 1):\n",
    "                similar_context += f\"\\n--- Caso Similar #{i} (similitud: {1-dist:.2f}) ---\\n\"\n",
    "                similar_context += f\"Resultado: {meta.get('final_result', 'N/A')}\\n\"\n",
    "                if meta.get('failure_step'):\n",
    "                    similar_context += f\"Paso de fallo: {meta['failure_step']}\\n\"\n",
    "                    similar_context += f\"Raz√≥n: {meta.get('failure_reason', 'N/A')}\\n\"\n",
    "                similar_context += \"\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"Eres un experto en an√°lisis de sistemas IVR (Interactive Voice Response) y patrones de fallo en llamadas telef√≥nicas self-service.\n",
    "\n",
    "Tu tarea es analizar una nueva llamada y explicar la causa probable del fallo bas√°ndote en:\n",
    "1. Los datos de la llamada actual\n",
    "2. Patrones encontrados en casos similares hist√≥ricos\n",
    "\n",
    "## NUEVA LLAMADA A ANALIZAR:\n",
    "{new_call['narrative']}\n",
    "\n",
    "## CASOS HIST√ìRICOS SIMILARES:\n",
    "{similar_context if similar_context else 'No se encontraron casos similares.'}\n",
    "\n",
    "## INSTRUCCIONES:\n",
    "Proporciona un an√°lisis estructurado que incluya:\n",
    "\n",
    "1. **DIAGN√ìSTICO**: Explica qu√© pas√≥ en esta llamada y por qu√© fall√≥.\n",
    "\n",
    "2. **PATR√ìN IDENTIFICADO**: Describe si este fallo sigue un patr√≥n com√∫n basado en los casos similares.\n",
    "\n",
    "3. **CAUSA RA√çZ PROBABLE**: Identifica la causa m√°s probable del fallo.\n",
    "\n",
    "4. **RECOMENDACIONES**: Sugiere mejoras espec√≠ficas para prevenir este tipo de fallos.\n",
    "\n",
    "5. **NIVEL DE CONFIANZA**: Indica qu√© tan seguro est√°s del diagn√≥stico (Alto/Medio/Bajo) y por qu√©.\n",
    "\n",
    "Responde en espa√±ol y s√© espec√≠fico con los nombres de pasos y errores.\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def analyze_call(self, call_data: Dict, find_similar: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Analiza una llamada y genera explicaci√≥n del fallo.\n",
    "        \"\"\"\n",
    "        # Procesar la nueva llamada\n",
    "        processor = CallDataProcessor(pd.DataFrame([call_data]))\n",
    "        processed = processor.process_all()[0]\n",
    "        \n",
    "        # Buscar casos similares\n",
    "        similar_cases = {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
    "        if find_similar:\n",
    "            similar_cases = self.vector_store.search_similar(\n",
    "                processed['narrative'],\n",
    "                n_results=config.TOP_K_SIMILAR,\n",
    "                filter_result=\"error\"  # Solo buscar entre fallos\n",
    "            )\n",
    "        \n",
    "        # Construir prompt\n",
    "        prompt = self._build_analysis_prompt(processed, similar_cases)\n",
    "        \n",
    "        # Generar an√°lisis con LLM\n",
    "        response = ollama.generate(\n",
    "            model=self.llm_model,\n",
    "            prompt=prompt,\n",
    "            options={\n",
    "                \"temperature\": 0.3,  # M√°s determin√≠stico para an√°lisis\n",
    "                \"num_predict\": 1000\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['response']\n",
    "    \n",
    "    def get_pattern_summary(self) -> str:\n",
    "        \"\"\"\n",
    "        Genera un resumen de patrones de fallo usando el LLM.\n",
    "        \"\"\"\n",
    "        patterns = self.vector_store.get_failure_patterns()\n",
    "        \n",
    "        prompt = f\"\"\"Analiza las siguientes estad√≠sticas de fallos en un sistema IVR y proporciona un resumen ejecutivo:\n",
    "\n",
    "## ESTAD√çSTICAS DE FALLOS:\n",
    "- Total de llamadas fallidas: {patterns['total_failures']}\n",
    "\n",
    "### Fallos por paso:\n",
    "{json.dumps(patterns['by_step'], indent=2)}\n",
    "\n",
    "### Fallos por raz√≥n:\n",
    "{json.dumps(patterns['by_reason'], indent=2)}\n",
    "\n",
    "### Acci√≥n final del usuario:\n",
    "{json.dumps(patterns['by_end_action'], indent=2)}\n",
    "\n",
    "Proporciona:\n",
    "1. Los 3 problemas m√°s cr√≠ticos\n",
    "2. Patrones preocupantes\n",
    "3. Recomendaciones priorizadas de mejora\n",
    "\n",
    "Responde en espa√±ol de forma concisa y accionable.\"\"\"\n",
    "        \n",
    "        response = ollama.generate(\n",
    "            model=self.llm_model,\n",
    "            prompt=prompt,\n",
    "            options={\"temperature\": 0.3}\n",
    "        )\n",
    "        \n",
    "        return response['response']\n",
    "\n",
    "# Inicializar analizador\n",
    "analyzer = IVRPatternAnalyzer(vector_store, config.LLM_MODEL)\n",
    "print(\"‚úÖ Analizador de patrones inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Uso del Sistema - Ejemplos Pr√°cticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver estad√≠sticas de patrones\n",
    "patterns = vector_store.get_failure_patterns()\n",
    "print(\"üìä ESTAD√çSTICAS DE PATRONES DE FALLO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTotal llamadas fallidas: {patterns['total_failures']}\")\n",
    "print(f\"\\nFallos por paso:\")\n",
    "for step, count in patterns['by_step'].items():\n",
    "    print(f\"   {step}: {count}\")\n",
    "print(f\"\\nFallos por raz√≥n:\")\n",
    "for reason, count in patterns['by_reason'].items():\n",
    "    print(f\"   {reason}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resumen ejecutivo con LLM\n",
    "print(\"üìã RESUMEN EJECUTIVO DE PATRONES\")\n",
    "print(\"=\" * 50)\n",
    "summary = analyzer.get_pattern_summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Analizar una Nueva Llamada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular una nueva llamada fallida para analizar\n",
    "nueva_llamada = {\n",
    "    \"call_id\": \"NEW_001\",\n",
    "    \"timestamp\": pd.Timestamp.now(),\n",
    "    \"steps_completed\": json.dumps([\n",
    "        \"welcome_message\",\n",
    "        \"language_selection\",\n",
    "        \"authentication\"\n",
    "    ]),\n",
    "    \"step_results\": json.dumps({\n",
    "        \"welcome_message\": \"success\",\n",
    "        \"language_selection\": \"success\",\n",
    "        \"authentication\": \"error\"\n",
    "    }),\n",
    "    \"final_result\": \"error\",\n",
    "    \"failure_step\": \"authentication\",\n",
    "    \"failure_reason\": \"auth_timeout\",\n",
    "    \"end_action\": \"transfer_agent\",\n",
    "    \"call_duration_seconds\": 45,\n",
    "    \"retry_count\": 2,\n",
    "    \"customer_segment\": \"premium\",\n",
    "    \"call_type\": \"billing\",\n",
    "    \"hour_of_day\": 14,\n",
    "    \"day_of_week\": \"miercoles\"\n",
    "}\n",
    "\n",
    "print(\"üîç ANALIZANDO NUEVA LLAMADA\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ID: {nueva_llamada['call_id']}\")\n",
    "print(f\"Tipo: {nueva_llamada['call_type']}\")\n",
    "print(f\"Fallo en: {nueva_llamada['failure_step']}\")\n",
    "print(f\"Raz√≥n: {nueva_llamada['failure_reason']}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"AN√ÅLISIS DEL LLM:\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "analisis = analyzer.analyze_call(nueva_llamada)\n",
    "print(analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Buscar Casos Similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar casos similares a un patr√≥n espec√≠fico\n",
    "query = \"llamada de billing que falla en autenticaci√≥n con timeout\"\n",
    "\n",
    "print(f\"üîé Buscando casos similares a: '{query}'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "resultados = vector_store.search_similar(\n",
    "    query_text=query,\n",
    "    n_results=3,\n",
    "    filter_result=\"error\"\n",
    ")\n",
    "\n",
    "for i, (doc, meta, dist) in enumerate(zip(\n",
    "    resultados['documents'][0],\n",
    "    resultados['metadatas'][0],\n",
    "    resultados['distances'][0]\n",
    "), 1):\n",
    "    print(f\"\\n--- Resultado #{i} (distancia: {dist:.4f}) ---\")\n",
    "    print(f\"Call ID: {meta['call_id']}\")\n",
    "    print(f\"Paso de fallo: {meta.get('failure_step', 'N/A')}\")\n",
    "    print(f\"Raz√≥n: {meta.get('failure_reason', 'N/A')}\")\n",
    "    print(f\"Tipo: {meta.get('call_type', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Funci√≥n Helper para An√°lisis R√°pido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_fallo_rapido(\n",
    "    call_type: str,\n",
    "    failure_step: str,\n",
    "    failure_reason: str,\n",
    "    customer_segment: str = \"standard\",\n",
    "    end_action: str = \"hangup\",\n",
    "    retries: int = 1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Funci√≥n helper para an√°lisis r√°pido de un caso de fallo.\n",
    "    \n",
    "    Ejemplo:\n",
    "        resultado = analizar_fallo_rapido(\n",
    "            call_type=\"billing\",\n",
    "            failure_step=\"authentication\",\n",
    "            failure_reason=\"biometric_fail\",\n",
    "            customer_segment=\"premium\"\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Construir pasos hasta el fallo\n",
    "    all_steps = [\n",
    "        \"welcome_message\",\n",
    "        \"language_selection\",\n",
    "        \"authentication\",\n",
    "        \"menu_principal\",\n",
    "        \"submenu_consultas\",\n",
    "        \"ingreso_datos\",\n",
    "        \"validacion_datos\",\n",
    "        \"procesamiento\"\n",
    "    ]\n",
    "    \n",
    "    # Encontrar √≠ndice del paso de fallo\n",
    "    if failure_step in all_steps:\n",
    "        fail_idx = all_steps.index(failure_step)\n",
    "        steps_completed = all_steps[:fail_idx + 1]\n",
    "    else:\n",
    "        steps_completed = [\"welcome_message\", failure_step]\n",
    "    \n",
    "    # Construir resultados\n",
    "    step_results = {step: \"success\" for step in steps_completed[:-1]}\n",
    "    step_results[failure_step] = \"error\"\n",
    "    \n",
    "    llamada = {\n",
    "        \"call_id\": \"QUICK_ANALYSIS\",\n",
    "        \"timestamp\": pd.Timestamp.now(),\n",
    "        \"steps_completed\": json.dumps(steps_completed),\n",
    "        \"step_results\": json.dumps(step_results),\n",
    "        \"final_result\": \"error\",\n",
    "        \"failure_step\": failure_step,\n",
    "        \"failure_reason\": failure_reason,\n",
    "        \"end_action\": end_action,\n",
    "        \"call_duration_seconds\": 60,\n",
    "        \"retry_count\": retries,\n",
    "        \"customer_segment\": customer_segment,\n",
    "        \"call_type\": call_type,\n",
    "        \"hour_of_day\": 12,\n",
    "        \"day_of_week\": \"miercoles\"\n",
    "    }\n",
    "    \n",
    "    return analyzer.analyze_call(llamada)\n",
    "\n",
    "print(\"‚úÖ Funci√≥n analizar_fallo_rapido() disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de la funci√≥n r√°pida\n",
    "print(\"üöÄ AN√ÅLISIS R√ÅPIDO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "resultado = analizar_fallo_rapido(\n",
    "    call_type=\"support\",\n",
    "    failure_step=\"ingreso_datos\",\n",
    "    failure_reason=\"speech_not_recognized\",\n",
    "    customer_segment=\"basic\",\n",
    "    end_action=\"transfer_agent\",\n",
    "    retries=3\n",
    ")\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exportar y Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el dataset de ejemplo para referencia\n",
    "df.to_csv(\"ivr_calls_sample.csv\", index=False)\n",
    "print(\"‚úÖ Dataset guardado en 'ivr_calls_sample.csv'\")\n",
    "\n",
    "# Guardar patrones identificados\n",
    "patterns = vector_store.get_failure_patterns()\n",
    "with open(\"failure_patterns.json\", \"w\") as f:\n",
    "    json.dump(patterns, f, indent=2)\n",
    "print(\"‚úÖ Patrones guardados en 'failure_patterns.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Resumen de Uso\n",
    "\n",
    "### Componentes principales:\n",
    "\n",
    "1. **`CallDataProcessor`**: Convierte datos CSV en narrativas textuales\n",
    "2. **`OllamaEmbeddings`**: Genera embeddings usando modelo local\n",
    "3. **`IVRVectorStore`**: Almacena y busca en ChromaDB\n",
    "4. **`IVRPatternAnalyzer`**: Analiza patrones con LLM\n",
    "\n",
    "### Flujo t√≠pico:\n",
    "\n",
    "```python\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv(\"tu_archivo.csv\")\n",
    "\n",
    "# 2. Procesar e indexar\n",
    "processor = CallDataProcessor(df)\n",
    "processed = processor.process_all()\n",
    "vector_store.add_calls(processed)\n",
    "\n",
    "# 3. Analizar nueva llamada\n",
    "resultado = analyzer.analyze_call(nueva_llamada)\n",
    "\n",
    "# 4. O usar funci√≥n r√°pida\n",
    "resultado = analizar_fallo_rapido(\n",
    "    call_type=\"billing\",\n",
    "    failure_step=\"authentication\",\n",
    "    failure_reason=\"timeout\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Modelos recomendados para Ollama:\n",
    "\n",
    "- **Embeddings**: `nomic-embed-text` (r√°pido y efectivo)\n",
    "- **LLM**: `llama3.1` o `mistral` (buenos para an√°lisis en espa√±ol)\n",
    "- **Alternativa ligera**: `phi3` o `gemma2` si tienes recursos limitados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
